{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function ##\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load clean dictionnary\n",
    "\n",
    "We trained the model, the model was trained local, there not the fully dictionary has been loaded but only 40000 translation French-English (could be single word to sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load all the data\n",
    "all_data = load_clean_sentences(\"english-french_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# amount of row selected\n",
    "sentences = 40000\n",
    "\n",
    "#get the data\n",
    "data = all_data[:sentences,:]\n",
    "shuffle(data)\n",
    "\n",
    "#split train and test\n",
    "\n",
    "train, test = data[:39000], data[39000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_final.pkl\n",
      "Saved: train_final.pkl\n",
      "Saved: test_final.pkl\n"
     ]
    }
   ],
   "source": [
    "#save data once more for safety\n",
    "\n",
    "save_clean_data(data, 'data_final.pkl')\n",
    "save_clean_data(train, 'train_final.pkl')\n",
    "save_clean_data(test, 'test_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the data for modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenized data4\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 5373\n",
      "English Max Length: 7\n"
     ]
    }
   ],
   "source": [
    "#english tokenizer\n",
    "eng_tok = create_tokenizer(data[:,0])\n",
    "eng_voc_size = len(eng_tok.word_index)+1\n",
    "eng_length = max_length(data[:,0])\n",
    "print('English Vocabulary Size: %d' % eng_voc_size)\n",
    "print('English Max Length: %d' % (eng_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary Size: 10818\n",
      "French Max Length: 14\n"
     ]
    }
   ],
   "source": [
    "#french tokenizer\n",
    "fra_tok = create_tokenizer(data[:,1])\n",
    "fra_voc_size = len(fra_tok.word_index)+1\n",
    "fra_length = max_length(data[:,1])\n",
    "print('French Vocabulary Size: %d' % fra_voc_size)\n",
    "print('French Max Length: %d' % (fra_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/eng_tok_final.pkl\n",
      "Saved: C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/eng_voc_size_final.pkl\n",
      "Saved: C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/eng_length_final.pkl\n",
      "Saved: C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/fra_tok_final.pkl\n",
      "Saved: C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/fra_voc_size_final.pkl\n",
      "Saved: C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/fra_length_final.pkl\n"
     ]
    }
   ],
   "source": [
    "#set path\n",
    "path = 'C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/'\n",
    "\n",
    "#save data\n",
    "save_clean_data(eng_tok,path + 'eng_tok_final.pkl')\n",
    "save_clean_data(eng_voc_size,path + 'eng_voc_size_final.pkl')\n",
    "save_clean_data(eng_length,path + 'eng_length_final.pkl')\n",
    "\n",
    "save_clean_data(fra_tok,path + 'fra_tok_final.pkl')\n",
    "save_clean_data(fra_voc_size,path + 'fra_voc_size_final.pkl')\n",
    "save_clean_data(fra_length,path + 'fra_length_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "Encoder-Decoder LSTM model with 3 layers and drop out of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encode train and test data\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "trainX = encode_sequences(fra_tok,fra_length, train[:,1])\n",
    "trainY = encode_sequences(eng_tok,eng_length, train[:,0])\n",
    "trainY = encode_output(trainY,eng_voc_size)\n",
    "\n",
    "testX = encode_sequences(fra_tok,fra_length, test[:,1])\n",
    "testY = encode_sequences(eng_tok,eng_length, test[:,0])\n",
    "testY = encode_output(testY,eng_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 14, 256)           2769408   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 7, 5373)           1380861   \n",
      "=================================================================\n",
      "Total params: 5,726,205\n",
      "Trainable params: 5,726,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 39000 samples, validate on 1000 samples\n",
      "Epoch 1/40\n",
      " - 491s - loss: 3.6480 - val_loss: 3.2944\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.29441, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 2/40\n",
      " - 444s - loss: 3.2133 - val_loss: 3.1511\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.29441 to 3.15109, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 3/40\n",
      " - 485s - loss: 3.0249 - val_loss: 2.9593\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.15109 to 2.95934, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 4/40\n",
      " - 523s - loss: 2.7975 - val_loss: 2.7285\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.95934 to 2.72846, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 5/40\n",
      " - 590s - loss: 2.5435 - val_loss: 2.4910\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.72846 to 2.49096, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 6/40\n",
      " - 591s - loss: 2.2900 - val_loss: 2.2922\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.49096 to 2.29225, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 7/40\n",
      " - 498s - loss: 2.0706 - val_loss: 2.1328\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.29225 to 2.13275, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 8/40\n",
      " - 444s - loss: 1.8865 - val_loss: 1.9834\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.13275 to 1.98338, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 9/40\n",
      " - 462s - loss: 1.7270 - val_loss: 1.8839\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.98338 to 1.88395, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 10/40\n",
      " - 445s - loss: 1.5816 - val_loss: 1.7898\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.88395 to 1.78978, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 11/40\n",
      " - 452s - loss: 1.4523 - val_loss: 1.7110\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.78978 to 1.71097, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 12/40\n",
      " - 461s - loss: 1.3332 - val_loss: 1.6341\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.71097 to 1.63414, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 13/40\n",
      " - 456s - loss: 1.2276 - val_loss: 1.5800\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.63414 to 1.57997, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 14/40\n",
      " - 459s - loss: 1.1371 - val_loss: 1.5348\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.57997 to 1.53484, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 15/40\n",
      " - 458s - loss: 1.0525 - val_loss: 1.4820\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.53484 to 1.48196, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 16/40\n",
      " - 459s - loss: 0.9767 - val_loss: 1.4477\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.48196 to 1.44772, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 17/40\n",
      " - 460s - loss: 0.9093 - val_loss: 1.4207\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.44772 to 1.42069, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 18/40\n",
      " - 455s - loss: 0.8488 - val_loss: 1.3991\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.42069 to 1.39913, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 19/40\n",
      " - 470s - loss: 0.7948 - val_loss: 1.3768\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.39913 to 1.37684, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 20/40\n",
      " - 469s - loss: 0.7477 - val_loss: 1.3716\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.37684 to 1.37163, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 21/40\n",
      " - 476s - loss: 0.7052 - val_loss: 1.3549\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.37163 to 1.35487, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 22/40\n",
      " - 463s - loss: 0.6642 - val_loss: 1.3498\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.35487 to 1.34979, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 23/40\n",
      " - 468s - loss: 0.6313 - val_loss: 1.3382\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.34979 to 1.33815, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 24/40\n",
      " - 588s - loss: 0.5976 - val_loss: 1.3300\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.33815 to 1.33004, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 25/40\n",
      " - 497s - loss: 0.5711 - val_loss: 1.3319\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/40\n",
      " - 472s - loss: 0.5434 - val_loss: 1.3222\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.33004 to 1.32216, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/model_final.h5\n",
      "Epoch 27/40\n",
      " - 465s - loss: 0.5220 - val_loss: 1.3298\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/40\n",
      " - 471s - loss: 0.5028 - val_loss: 1.3630\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/40\n",
      " - 462s - loss: 0.4815 - val_loss: 1.3559\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/40\n",
      " - 505s - loss: 0.4660 - val_loss: 1.3469\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/40\n",
      " - 485s - loss: 0.4474 - val_loss: 1.3414\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/40\n",
      " - 456s - loss: 0.4299 - val_loss: 1.3443\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/40\n",
      " - 484s - loss: 0.4187 - val_loss: 1.3344\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/40\n",
      " - 454s - loss: 0.4020 - val_loss: 1.3589\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/40\n",
      " - 489s - loss: 0.3892 - val_loss: 1.3543\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/40\n",
      " - 476s - loss: 0.3807 - val_loss: 1.3711\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/40\n",
      " - 479s - loss: 0.3689 - val_loss: 1.3798\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/40\n",
      " - 471s - loss: 0.3598 - val_loss: 1.3804\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/40\n",
      " - 455s - loss: 0.3485 - val_loss: 1.3766\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/40\n",
      " - 479s - loss: 0.3412 - val_loss: 1.3620\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b2d26d630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(Dropout(0.2, noise_shape=None, seed=42))\n",
    "   # model.add(LSTM(180))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "   # model.add(LSTM(n_units, return_sequences=False))\n",
    "    model.add(LSTM(n_units,return_sequences=True))\n",
    "    model.add(Dropout(0.2, noise_shape=None, seed=42))\n",
    "\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(Dropout(0.2, noise_shape=None, seed=42))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "# define model\n",
    "model = define_model(fra_voc_size, eng_voc_size, fra_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = path+'model_final.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=40, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load save model\n",
    "model = load_model(path+'model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to get back the original nature of the date (meaning text) for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model result with BLEU (bilingual evaluation understudy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tok, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[voulezvous rester ], target=[do you want to stay], predicted=[do you want to stay]\n",
      "src=[ils te craignaient], target=[they feared you], predicted=[they feared you]\n",
      "src=[je le fais beaucoup], target=[i do that a lot], predicted=[i do it a lot]\n",
      "src=[vous fiezvous a sa parole ], target=[do you believe him], predicted=[do you see him]\n",
      "src=[il n'en sait encore rien], target=[he doesn't know yet], predicted=[he doesn't not yet]\n",
      "src=[ou sontils ], target=[where are they], predicted=[where are they]\n",
      "src=[peuxtu monter un cheval ], target=[can you ride a horse], predicted=[can you ride a horse]\n",
      "src=[nous etudions le francais], target=[we study french], predicted=[we study french]\n",
      "src=[il est capable de courir vite], target=[he can run fast], predicted=[he can run fast]\n",
      "src=[je n'ai eu aucun doute], target=[i had no doubts], predicted=[i had no doubts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viret\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.084625\n",
      "BLEU-2: 0.288248\n",
      "BLEU-3: 0.470625\n",
      "BLEU-4: 0.531988\n",
      "test\n",
      "src=[vous semblez occupees], target=[you seem busy], predicted=[you seem busy]\n",
      "src=[je ne ronfle pas], target=[i don't snore], predicted=[i didn't gamble]\n",
      "src=[c'est un etudiant], target=[he is a student], predicted=[he is a student]\n",
      "src=[pars ], target=[go away], predicted=[go away]\n",
      "src=[ne vous retournez pas], target=[don't turn around], predicted=[let's die at]\n",
      "src=[je veux un beignet], target=[i want a donut], predicted=[i want a divorce]\n",
      "src=[j'ai donne un livre a mary], target=[i gave mary a book], predicted=[i gave a water]\n",
      "src=[j'allais partir], target=[i was going to go], predicted=[i was to to]\n",
      "src=[elle a l'air perplexe], target=[she looks confused], predicted=[she looks confused]\n",
      "src=[puisje manger ce gateau ], target=[can i eat this cake], predicted=[may i eat this cake]\n",
      "BLEU-1: 0.076723\n",
      "BLEU-2: 0.273682\n",
      "BLEU-3: 0.455172\n",
      "BLEU-4: 0.516901\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tok, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tok, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [\"tu me manques\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tu me manques']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  18   20 1417    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "testUN = encode_sequences(fra_tok,fra_length, text)\n",
    "print(testUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i miss you'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply model prediction\n",
    "predict_sequence(model,eng_tok,testUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Bleu score is good and the translation seems to work fine â€“ of course, since it has been trained locally and with a relatively small dictionary, they will be some mis translation and the sentence are not to elaborated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
