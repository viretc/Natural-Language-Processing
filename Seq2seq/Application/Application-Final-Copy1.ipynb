{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import re\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from emo_utils import *\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# tokenized data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "   \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (‚âà 1 line)\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = (X[i].lower()).split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j+1\n",
    "            \n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def application(text):\n",
    "    try:\n",
    "        \n",
    "        text_enc = encode_sequences(fra_tok,fra_length,text)\n",
    "        prediction = predict_sequence(seq2seq,eng_tok, text_enc)\n",
    "        prediction = decontracted(prediction) \n",
    "        print(\"Transaltion:\", ' '.join(map(str, text)), \"==>\",prediction)\n",
    "        prediction = np.array([prediction])\n",
    "        X_test_indices = sentences_to_indices(prediction, word_to_index, maxLen)\n",
    "        print(\"Adding emoji ==>\", prediction[0] +' '+  label_to_emoji(np.argmax(model_1.predict(X_test_indices))))\n",
    "    except:\n",
    "        print(\"No emoji for the moment\")\n",
    "\n",
    "def application2(text):\n",
    "    try:\n",
    "        text_enc = encode_sequences(fra_tok,fra_length,text)\n",
    "        prediction = predict_sequence(seq2seq,eng_tok, text_enc)\n",
    "        prediction = decontracted(prediction) \n",
    "        print(\"Transaltion:\", ' '.join(map(str, text)), \"==>\",prediction)\n",
    "        prediction = np.array([prediction])\n",
    "        X_test_indices = sentences_to_indices(prediction, word_to_index, maxLen)\n",
    "        L = np.argsort(-(model_1.predict((X_test_indices))), axis=1).reshape(5)\n",
    "        print(\"Adding emojis ==>\", prediction[0] +' '+   label_to_emoji(L[0]) + ' ' + label_to_emoji(L[1]) + ' ' + label_to_emoji(L[2]))\n",
    "    except:\n",
    "        print(\"No emoji for the moment\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load necessary data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fra_tok = load_clean_sentences('fra_tok_final.pkl')\n",
    "fra_length = load_clean_sentences('fra_length_final.pkl')\n",
    "eng_tok = load_clean_sentences('eng_tok_final.pkl')\n",
    "eng_lenght = load_clean_sentences('eng_length_final.pkl')\n",
    "word_to_index = load_clean_sentences('word_to_index_final.pkl')\n",
    "maxLen = load_clean_sentences('maxLen_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq = load_model('model_final.h5') #seq2seq model\n",
    "model_1 = load_model('emojis_final.h5') #lstm from emoji file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [\"j'adore le fromage\"],[\"Tu me manques\"],['ne soyez pas en colere'],[\"j'aime le football\"],[\"J'aime le soleil\"],[\"j'ai faim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanslation part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaltion: j'adore le fromage ==> i love sauerkraut\n",
      "Adding emoji ==> i love sauerkraut ‚ù§Ô∏è\n",
      "------\n",
      "Transaltion: Tu me manques ==> i miss you\n",
      "Adding emoji ==> i miss you ‚ù§Ô∏è\n",
      "------\n",
      "Transaltion: ne soyez pas en colere ==> do not be angry angry\n",
      "Adding emoji ==> do not be angry angry üòû\n",
      "------\n",
      "Transaltion: j'aime le football ==> i like soccer\n",
      "Adding emoji ==> i like soccer ‚öæ\n",
      "------\n",
      "Transaltion: J'aime le soleil ==> i like the sun\n",
      "Adding emoji ==> i like the sun üòÑ\n",
      "------\n",
      "Transaltion: j'ai faim ==> i am hungry hungry\n",
      "Adding emoji ==> i am hungry hungry üç¥\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(text)):\n",
    "    application(text[i])\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**When there is small simple sentence to translate from French to English, the model preformed well 3 out of 6 sentence are 100% correctly translated ‚Äì 2 are properly translated even if the last word is repeated twice (so far I could not really find out why) and the last sentence is somehow correct, but it should have been ‚Äúcheese‚Äù instead of sauerkraut ‚Äì most probably the issue come from the very small dictionary used to train the model.**\n",
    "\n",
    "**There is a lot of room for improvement, such as incorporating GLOVE or World2Vec to the embedded layer of the seq2seq model ‚Äì could also have done similar process with a French version ‚Äì increase the dictionary size and train the model with GPU instead of locally with CPU.\n",
    "Link of our presentation and use-cases: **https://slides.com/anniepi/deck#/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
