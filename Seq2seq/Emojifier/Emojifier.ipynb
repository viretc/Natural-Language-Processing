{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Dataset EMOJISET\n",
    "\n",
    "Let's start by building a simple baseline classifier. \n",
    "\n",
    "You have a tiny dataset (X, Y) where:\n",
    "- X contains 127 sentences (strings)\n",
    "- Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "<img src=\"data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> *Figure 1*: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n",
    "Let's load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from pickle import load, dump\n",
    "#import pandas as pd\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Glove for synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train= read_csv('train_emoji_rework.csv')\n",
    "X_test, Y_test = read_csv('tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=len).split())+3\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "#set path\n",
    "path = 'C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/'\n",
    "\n",
    "#Save word_to_index\n",
    "with open( path + 'word_to_index_final.pkl', 'wb') as f:\n",
    "    dump(word_to_index, f)\n",
    "    print(\"saved\")\n",
    "\n",
    "#saved maxlen\n",
    "with open( path + 'maxLen_final.pkl', 'wb') as f:\n",
    "    dump(maxLen, f)\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Overview of the model\n",
    "\n",
    "\n",
    "<img src=\"emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 3**: Emojifier-V2. A 2-layer LSTM sequence classifier. </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):  #decontrat specific words\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):  #transfrom the sentence into an indices for the LSTM\n",
    "    \n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (‚âà 1 line)\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "       \n",
    "\n",
    "        sentence_words = (decontracted(X[i].lower()).split())\n",
    "       # sentence_words = \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "\n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "                #  sentence_words = X[i].decode('latin1')\n",
    "                # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "\n",
    "                # Increment j to j + 1\n",
    "                j = j+1\n",
    "             \n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "   \n",
    "    \n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "\n",
    "def softMaxAxis1(x):\n",
    "    return softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Emojify_V2\n",
    "\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(input_shape)\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (‚âà1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True) (embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5) (X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True) (X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5) (X)\n",
    "    X = LSTM(128) (X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(5)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    #X=Lambda(lambda x: K.tf.nn.softmax(x))\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 13, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 13, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 13, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,355,511\n",
      "Trainable params: 355,461\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 0s - loss: 0.2911 - acc: 0.8784\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.29111, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.2508 - acc: 0.9099\n",
      "\n",
      "Epoch 00002: loss improved from 0.29111 to 0.25082, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.2729 - acc: 0.9054\n",
      "\n",
      "Epoch 00003: loss did not improve\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.2208 - acc: 0.9144\n",
      "\n",
      "Epoch 00004: loss improved from 0.25082 to 0.22080, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.1918 - acc: 0.9324\n",
      "\n",
      "Epoch 00005: loss improved from 0.22080 to 0.19176, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.1494 - acc: 0.9414\n",
      "\n",
      "Epoch 00006: loss improved from 0.19176 to 0.14944, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.1548 - acc: 0.9369\n",
      "\n",
      "Epoch 00007: loss did not improve\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.2011 - acc: 0.9279\n",
      "\n",
      "Epoch 00008: loss did not improve\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.1776 - acc: 0.9369\n",
      "\n",
      "Epoch 00009: loss did not improve\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.1966 - acc: 0.9279\n",
      "\n",
      "Epoch 00010: loss did not improve\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.1937 - acc: 0.9324\n",
      "\n",
      "Epoch 00011: loss did not improve\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.1161 - acc: 0.9595\n",
      "\n",
      "Epoch 00012: loss improved from 0.14944 to 0.11606, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.1030 - acc: 0.9730\n",
      "\n",
      "Epoch 00013: loss improved from 0.11606 to 0.10298, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.1366 - acc: 0.9550\n",
      "\n",
      "Epoch 00014: loss did not improve\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.1396 - acc: 0.9459\n",
      "\n",
      "Epoch 00015: loss did not improve\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.1994 - acc: 0.9279\n",
      "\n",
      "Epoch 00016: loss did not improve\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.2527 - acc: 0.9189\n",
      "\n",
      "Epoch 00017: loss did not improve\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.2052 - acc: 0.9595\n",
      "\n",
      "Epoch 00018: loss did not improve\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.1560 - acc: 0.9369\n",
      "\n",
      "Epoch 00019: loss did not improve\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.0874 - acc: 0.9865\n",
      "\n",
      "Epoch 00020: loss improved from 0.10298 to 0.08739, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.0764 - acc: 0.9820\n",
      "\n",
      "Epoch 00021: loss improved from 0.08739 to 0.07636, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0642 - acc: 0.9775\n",
      "\n",
      "Epoch 00022: loss improved from 0.07636 to 0.06425, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0729 - acc: 0.9730\n",
      "\n",
      "Epoch 00023: loss did not improve\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0589 - acc: 0.9775\n",
      "\n",
      "Epoch 00024: loss improved from 0.06425 to 0.05891, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0407 - acc: 0.9865\n",
      "\n",
      "Epoch 00025: loss improved from 0.05891 to 0.04073, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0319 - acc: 0.9910\n",
      "\n",
      "Epoch 00026: loss improved from 0.04073 to 0.03186, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0417 - acc: 0.9820\n",
      "\n",
      "Epoch 00027: loss did not improve\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0183 - acc: 0.9955\n",
      "\n",
      "Epoch 00028: loss improved from 0.03186 to 0.01833, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0232 - acc: 0.9910\n",
      "\n",
      "Epoch 00029: loss did not improve\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0299 - acc: 0.9865\n",
      "\n",
      "Epoch 00030: loss did not improve\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0621 - acc: 0.9730\n",
      "\n",
      "Epoch 00031: loss did not improve\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0951 - acc: 0.9640\n",
      "\n",
      "Epoch 00032: loss did not improve\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.1715 - acc: 0.9505\n",
      "\n",
      "Epoch 00033: loss did not improve\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0795 - acc: 0.9730\n",
      "\n",
      "Epoch 00034: loss did not improve\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.1070 - acc: 0.9595\n",
      "\n",
      "Epoch 00035: loss did not improve\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.1129 - acc: 0.9685\n",
      "\n",
      "Epoch 00036: loss did not improve\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.1180 - acc: 0.9640\n",
      "\n",
      "Epoch 00037: loss did not improve\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.0649 - acc: 0.9865\n",
      "\n",
      "Epoch 00038: loss did not improve\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0293 - acc: 0.9910\n",
      "\n",
      "Epoch 00039: loss did not improve\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.0316 - acc: 0.9955\n",
      "\n",
      "Epoch 00040: loss did not improve\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.0216 - acc: 0.9955\n",
      "\n",
      "Epoch 00041: loss did not improve\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.0115 - acc: 1.0000\n",
      "\n",
      "Epoch 00042: loss improved from 0.01833 to 0.01153, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0083 - acc: 1.0000\n",
      "\n",
      "Epoch 00043: loss improved from 0.01153 to 0.00833, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0061 - acc: 1.0000\n",
      "\n",
      "Epoch 00044: loss improved from 0.00833 to 0.00612, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0045 - acc: 1.0000\n",
      "\n",
      "Epoch 00045: loss improved from 0.00612 to 0.00447, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0041 - acc: 1.0000\n",
      "\n",
      "Epoch 00046: loss improved from 0.00447 to 0.00413, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0028 - acc: 1.0000\n",
      "\n",
      "Epoch 00047: loss improved from 0.00413 to 0.00283, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0024 - acc: 1.0000\n",
      "\n",
      "Epoch 00048: loss improved from 0.00283 to 0.00243, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0025 - acc: 1.0000\n",
      "\n",
      "Epoch 00049: loss did not improve\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0020 - acc: 1.0000\n",
      "\n",
      "Epoch 00050: loss improved from 0.00243 to 0.00205, saving model to C:/Users/viret/OneDrive/IE/Third_Term/NLP/Application/emojis_final2.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27bd53c30b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = path + 'emojis_final2.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True, callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load save model\n",
    "model = load_model(path + 'emojis_final2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 18ms/step\n",
      "\n",
      "Test accuracy =  0.767857151372\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòÑ prediction: We had such a lovely dinner tonight\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is hard\tüòÑ\n",
      "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: are you serious‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is horrible\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: stop pissing me off‚öæ\n",
      "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: My life is so boring\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: dance with me\t‚öæ\n",
      "Expected emoji:üòÑ prediction: What you did was awesome\tüòû\n",
      "Expected emoji:üòû prediction: go away\t‚öæ\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love baseball ‚öæ\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['i love baseball'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutiple Emojis\n",
    "\n",
    "based on the score received by the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love baseball ‚öæ üòû ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "#mutiple emojis\n",
    "\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "p=model.predict(X_test_indices).reshape(5)\n",
    "L = np.argsort(-(model.predict((X_test_indices))), axis=1).reshape(5)\n",
    "print(x_test[0] +' '+  label_to_emoji(L[0]) + ' ' + label_to_emoji(L[1]) + ' ' + label_to_emoji(L[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**It Is a really small dataset and the accuracy is somehow decent but nothing amazing ‚Äì the aim of the project was not so much to work on the emoji classification but more and the NLP seq2seq translation and create a prototype.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
